{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Notes:\n",
    "\n",
    "1. The first thing to know about is a \"Byte Pair encoding\" (BPE). According to huggingface [this does not actually *necessarily* involves bytes](https://huggingface.co/docs/transformers/tokenizer_summary#bytepair-encoding-bpe). Rather, given a corpus $C$ and a base vocabulary $V$ such that $\\forall c \\in C, \\exists (v_1, ..., v_k) \\in V^\\star s.t. c=v_1\\cdots v_k$. The tokenizer will then find the most frequent pair of symbols $\\in V$ and merge them, then re-evaluate frequencies with the new $V'$. This repeats until our vocabulary has a desired small enough size.\n",
    "2. Byte-Level BPE is a special case of the \"Byte Pair Encoding\" where the symbols are bytes, that is $v_1 = 0b00000000, v_2 = 0b00000001, ..., v_{256} = 0b11111111$. Every unicode character is a sequence of unicode bytes. For example, \"üêπ\" is `0xF0 9F 90 B9`. This means that in theory *any* character can be represented by our model, but the tokens might look really strange to us (see below).  \n",
    "3. [In the DeBERTa paper](https://arxiv.org/pdf/2006.03654.pdf) they say that they use the BPE vocabulary of [Radford](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) and [Liu](https://arxiv.org/pdf/1907.11692.pdf). Liu just references Radford (section 4.4), and in Radford they say:\n",
    "\n",
    "```\n",
    "Byte Pair Encoding (BPE) (Sennrich et al., 2015) is a\n",
    "practical middle ground between character and word level\n",
    "language modeling which effectively interpolates between\n",
    "word level inputs for frequent symbol sequences and character level inputs for infrequent symbol sequences. Despite\n",
    "its name, reference BPE implementations often operate on\n",
    "Unicode code points and not byte sequences. These implementations would require including the full space of Unicode symbols in order to model all Unicode strings. This\n",
    "would result in a base vocabulary of over 130,000 before\n",
    "any multi-symbol tokens are added. This is prohibitively\n",
    "large compared to the 32,000 to 64,000 token vocabularies\n",
    "often used with BPE. In contrast, a byte-level version of\n",
    "BPE only requires a base vocabulary of size 256. However,\n",
    "directly applying BPE to the byte sequence results in suboptimal merges due to BPE using a greedy frequency based\n",
    "heuristic for building the token vocabulary. We observed\n",
    "BPE including many versions of common words like dog\n",
    "since they occur in many variations such as dog. dog!\n",
    "dog? . This results in a sub-optimal allocation of limited\n",
    "vocabulary slots and model capacity. To avoid this, we prevent BPE from merging across character categories for any\n",
    "byte sequence. We add an exception for spaces which significantly improves the compression efficiency while adding\n",
    "only minimal fragmentation of words across multiple vocab\n",
    "token\n",
    "```\n",
    "The unicode categories they refer to I believe are [these](https://www.fileformat.info/info/unicode/category/index.htm)\n",
    "\n",
    "So what is the conclusion here? We don't need to worry about out of vocabulary words, but our tokens might be pretty challenging to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, PreTrainedTokenizer\n",
    "tokenizer:PreTrainedTokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['√∞≈Å', '¬¶', 'ƒª']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer.tokenize('ü¶ô')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "59811a21b6bdb79b6f1a3b21d01b6e64bb63c5c292a63d3c5cec461d0e515581"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('PersonalCoding')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
