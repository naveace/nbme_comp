{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stepping through the theoviel notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "multiprocessing.set_start_method(\"fork\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=true\n",
      "tokenizers.__version__: 0.10.3\n",
      "transformers.__version__: 4.16.2\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import string\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "import itertools\n",
    "import warnings\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import tokenizers\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n",
    "print(f\"transformers.__version__: {transformers.__version__}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "from typing import List, Union, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    wandb=False\n",
    "    competition='NBME'\n",
    "    _wandb_kernel='nakama'\n",
    "    debug=False\n",
    "    apex=True\n",
    "    print_freq=100\n",
    "    num_workers=4\n",
    "    model=\"microsoft/deberta-base\"\n",
    "    scheduler='cosine' # ['linear', 'cosine']\n",
    "    batch_scheduler=True\n",
    "    num_cycles=0.5\n",
    "    num_warmup_steps=0\n",
    "    epochs=5\n",
    "    encoder_lr=2e-5\n",
    "    decoder_lr=2e-5\n",
    "    min_lr=1e-6\n",
    "    eps=1e-6\n",
    "    betas=(0.9, 0.999)\n",
    "    batch_size=12\n",
    "    fc_dropout=0.2\n",
    "    max_len=512\n",
    "    weight_decay=0.01\n",
    "    gradient_accumulation_steps=1\n",
    "    max_grad_norm=1000\n",
    "    seed=42\n",
    "    n_fold=5\n",
    "    trn_fold=[0, 1, 2, 3, 4]\n",
    "    train=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are all operations for computing f1 score on the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://www.kaggle.com/theoviel/evaluation-metric-folds-baseline\n",
    "\n",
    "def micro_f1(preds: List[List[int]], truths: List[List[int]]) -> float:\n",
    "    \"\"\"\n",
    "    Micro f1 on binary arrays.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of ints): Predictions.\n",
    "        truths (list of lists of ints): Ground truths.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    # Micro : aggregating over all instances\n",
    "    preds = np.concatenate(preds)\n",
    "    truths = np.concatenate(truths)\n",
    "    return f1_score(truths, preds)\n",
    "\n",
    "\n",
    "def spans_to_binary(spans: List[List[int]], length=None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Converts spans to a binary array indicating whether each character is in the span.\n",
    "\n",
    "    Args:\n",
    "        spans (list of lists of two ints): Spans.\n",
    "\n",
    "    Returns:\n",
    "        np array [length]: Binarized spans.\n",
    "    \"\"\"\n",
    "    length:int = np.max(spans) if length is None else length\n",
    "    binary = np.zeros(length)\n",
    "    for start, end in spans:\n",
    "        binary[start:end] = 1\n",
    "    return binary\n",
    "\n",
    "\n",
    "def span_micro_f1(preds: List[List[int]], truths: List[List[int]]) -> float:\n",
    "    \"\"\"\n",
    "    Micro f1 on spans.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of two ints): Prediction spans.\n",
    "        truths (list of lists of two ints): Ground truth spans.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    bin_preds = []\n",
    "    bin_truths = []\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if not len(pred) and not len(truth):\n",
    "            continue\n",
    "        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n",
    "        bin_preds.append(spans_to_binary(pred, length))\n",
    "        bin_truths.append(spans_to_binary(truth, length))\n",
    "    return micro_f1(bin_preds, bin_truths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are functions for taking predictions and mapping them to results which can then be scored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels_for_scoring(df: pd.DataFrame):\n",
    "    # example: ['0 1', '3 4'] -> ['0 1; 3 4']\n",
    "    df['location_for_create_labels'] = [ast.literal_eval(f'[]')] * len(df)\n",
    "    for i in range(len(df)):\n",
    "        lst = df.loc[i, 'location']\n",
    "        if lst:\n",
    "            new_lst = ';'.join(lst)\n",
    "            df.loc[i, 'location_for_create_labels'] = ast.literal_eval(f'[[\"{new_lst}\"]]')\n",
    "    # create labels\n",
    "    truths = []\n",
    "    for location_list in df['location_for_create_labels'].values:\n",
    "        truth = []\n",
    "        if len(location_list) > 0:\n",
    "            location = location_list[0]\n",
    "            for loc in [s.split() for s in location.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                truth.append([start, end])\n",
    "        truths.append(truth)\n",
    "    return truths\n",
    "\n",
    "\n",
    "def get_char_probs(texts, predictions, tokenizer):\n",
    "    results = [np.zeros(len(t)) for t in texts]\n",
    "    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n",
    "        encoded = tokenizer(text, \n",
    "                            add_special_tokens=True,\n",
    "                            return_offsets_mapping=True)\n",
    "        for idx, (offset_mapping, pred) in enumerate(zip(encoded['offset_mapping'], prediction)):\n",
    "            start = offset_mapping[0]\n",
    "            end = offset_mapping[1]\n",
    "            results[i][start:end] = pred\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_results(char_probs, th=0.5):\n",
    "    results = []\n",
    "    for char_prob in char_probs:\n",
    "        result = np.where(char_prob >= th)[0] + 1\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        result = [f\"{min(r)} {max(r)}\" for r in result]\n",
    "        result = \";\".join(result)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_predictions(results):\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are utils for the score, logging, and seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Utils\n",
    "# ====================================================\n",
    "def get_score(y_true, y_pred):\n",
    "    score = span_micro_f1(y_true, y_pred)\n",
    "    return score\n",
    "\n",
    "\n",
    "def get_logger(filename='train'):\n",
    "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = get_logger()\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe these are all the same as the `get_clean_train_data` function that I made. Would be good to have a second pair of eyes verify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape: (14300, 6)\n",
      "          id  case_num  pn_num  feature_num  \\\n",
      "0  00016_000         0      16            0   \n",
      "1  00016_001         0      16            1   \n",
      "2  00016_002         0      16            2   \n",
      "3  00016_003         0      16            3   \n",
      "4  00016_004         0      16            4   \n",
      "\n",
      "                               annotation          location  \n",
      "0          [dad with recent heart attcak]         [696 724]  \n",
      "1             [mom with \"thyroid disease]         [668 693]  \n",
      "2                        [chest pressure]         [203 217]  \n",
      "3        [intermittent episodes, episode]  [70 91, 176 183]  \n",
      "4  [felt as if he were going to pass out]         [222 258]  \n",
      "features.shape: (143, 3)\n",
      "   feature_num  case_num                                       feature_text\n",
      "0            0         0  Family-history-of-MI-OR-Family-history-of-myoc...\n",
      "1            1         0                 Family-history-of-thyroid-disorder\n",
      "2            2         0                                     Chest-pressure\n",
      "3            3         0                              Intermittent-symptoms\n",
      "4            4         0                                        Lightheaded\n",
      "patient_notes.shape: (42146, 3)\n",
      "   pn_num  case_num                                         pn_history\n",
      "0       0         0  17-year-old male, has come to the student heal...\n",
      "1       1         0  17 yo male with recurrent palpitations for the...\n",
      "2       2         0  Dillon Cleveland is a 17 y.o. male patient wit...\n",
      "3       3         0  a 17 yo m c/o palpitation started 3 mos ago; \\...\n",
      "4       4         0  17yo male with no pmh here for evaluation of p...\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Data Loading\n",
    "# ====================================================\n",
    "train = pd.read_csv('../data/train.csv')\n",
    "train['annotation'] = train['annotation'].apply(ast.literal_eval)\n",
    "train['location'] = train['location'].apply(ast.literal_eval)\n",
    "features = pd.read_csv('../data/features.csv')\n",
    "def preprocess_features(features):\n",
    "    features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n",
    "    return features\n",
    "features = preprocess_features(features)\n",
    "patient_notes = pd.read_csv('../data/patient_notes.csv')\n",
    "\n",
    "print(f\"train.shape: {train.shape}\")\n",
    "print(train.head())\n",
    "print(f\"features.shape: {features.shape}\")\n",
    "print(features.head())\n",
    "print(f\"patient_notes.shape: {patient_notes.shape}\")\n",
    "print(patient_notes.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id  case_num  pn_num  feature_num  \\\n",
      "0  00016_000         0      16            0   \n",
      "1  00016_001         0      16            1   \n",
      "2  00016_002         0      16            2   \n",
      "3  00016_003         0      16            3   \n",
      "4  00016_004         0      16            4   \n",
      "\n",
      "                               annotation          location  \\\n",
      "0          [dad with recent heart attcak]         [696 724]   \n",
      "1             [mom with \"thyroid disease]         [668 693]   \n",
      "2                        [chest pressure]         [203 217]   \n",
      "3        [intermittent episodes, episode]  [70 91, 176 183]   \n",
      "4  [felt as if he were going to pass out]         [222 258]   \n",
      "\n",
      "                                        feature_text  \\\n",
      "0  Family-history-of-MI-OR-Family-history-of-myoc...   \n",
      "1                 Family-history-of-thyroid-disorder   \n",
      "2                                     Chest-pressure   \n",
      "3                              Intermittent-symptoms   \n",
      "4                                        Lightheaded   \n",
      "\n",
      "                                          pn_history  \n",
      "0  HPI: 17yo M presents with palpitations. Patien...  \n",
      "1  HPI: 17yo M presents with palpitations. Patien...  \n",
      "2  HPI: 17yo M presents with palpitations. Patien...  \n",
      "3  HPI: 17yo M presents with palpitations. Patien...  \n",
      "4  HPI: 17yo M presents with palpitations. Patien...  \n"
     ]
    }
   ],
   "source": [
    "train = train.merge(features, on=['feature_num', 'case_num'], how='left')\n",
    "train = train.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incorrect annotation\n",
    "train.loc[338, 'annotation'] = ast.literal_eval('[[\"father heart attack\"]]')\n",
    "train.loc[338, 'location'] = ast.literal_eval('[[\"764 783\"]]')\n",
    "\n",
    "train.loc[621, 'annotation'] = ast.literal_eval('[[\"for the last 2-3 months\"]]')\n",
    "train.loc[621, 'location'] = ast.literal_eval('[[\"77 100\"]]')\n",
    "\n",
    "train.loc[655, 'annotation'] = ast.literal_eval('[[\"no heat intolerance\"], [\"no cold intolerance\"]]')\n",
    "train.loc[655, 'location'] = ast.literal_eval('[[\"285 292;301 312\"], [\"285 287;296 312\"]]')\n",
    "\n",
    "train.loc[1262, 'annotation'] = ast.literal_eval('[[\"mother thyroid problem\"]]')\n",
    "train.loc[1262, 'location'] = ast.literal_eval('[[\"551 557;565 580\"]]')\n",
    "\n",
    "train.loc[1265, 'annotation'] = ast.literal_eval('[[\\'felt like he was going to \"pass out\"\\']]')\n",
    "train.loc[1265, 'location'] = ast.literal_eval('[[\"131 135;181 212\"]]')\n",
    "\n",
    "train.loc[1396, 'annotation'] = ast.literal_eval('[[\"stool , with no blood\"]]')\n",
    "train.loc[1396, 'location'] = ast.literal_eval('[[\"259 280\"]]')\n",
    "\n",
    "train.loc[1591, 'annotation'] = ast.literal_eval('[[\"diarrhoe non blooody\"]]')\n",
    "train.loc[1591, 'location'] = ast.literal_eval('[[\"176 184;201 212\"]]')\n",
    "\n",
    "train.loc[1615, 'annotation'] = ast.literal_eval('[[\"diarrhea for last 2-3 days\"]]')\n",
    "train.loc[1615, 'location'] = ast.literal_eval('[[\"249 257;271 288\"]]')\n",
    "\n",
    "train.loc[1664, 'annotation'] = ast.literal_eval('[[\"no vaginal discharge\"]]')\n",
    "train.loc[1664, 'location'] = ast.literal_eval('[[\"822 824;907 924\"]]')\n",
    "\n",
    "train.loc[1714, 'annotation'] = ast.literal_eval('[[\"started about 8-10 hours ago\"]]')\n",
    "train.loc[1714, 'location'] = ast.literal_eval('[[\"101 129\"]]')\n",
    "\n",
    "train.loc[1929, 'annotation'] = ast.literal_eval('[[\"no blood in the stool\"]]')\n",
    "train.loc[1929, 'location'] = ast.literal_eval('[[\"531 539;549 561\"]]')\n",
    "\n",
    "train.loc[2134, 'annotation'] = ast.literal_eval('[[\"last sexually active 9 months ago\"]]')\n",
    "train.loc[2134, 'location'] = ast.literal_eval('[[\"540 560;581 593\"]]')\n",
    "\n",
    "train.loc[2191, 'annotation'] = ast.literal_eval('[[\"right lower quadrant pain\"]]')\n",
    "train.loc[2191, 'location'] = ast.literal_eval('[[\"32 57\"]]')\n",
    "\n",
    "train.loc[2553, 'annotation'] = ast.literal_eval('[[\"diarrhoea no blood\"]]')\n",
    "train.loc[2553, 'location'] = ast.literal_eval('[[\"308 317;376 384\"]]')\n",
    "\n",
    "train.loc[3124, 'annotation'] = ast.literal_eval('[[\"sweating\"]]')\n",
    "train.loc[3124, 'location'] = ast.literal_eval('[[\"549 557\"]]')\n",
    "\n",
    "train.loc[3858, 'annotation'] = ast.literal_eval('[[\"previously as regular\"], [\"previously eveyr 28-29 days\"], [\"previously lasting 5 days\"], [\"previously regular flow\"]]')\n",
    "train.loc[3858, 'location'] = ast.literal_eval('[[\"102 123\"], [\"102 112;125 141\"], [\"102 112;143 157\"], [\"102 112;159 171\"]]')\n",
    "\n",
    "train.loc[4373, 'annotation'] = ast.literal_eval('[[\"for 2 months\"]]')\n",
    "train.loc[4373, 'location'] = ast.literal_eval('[[\"33 45\"]]')\n",
    "\n",
    "train.loc[4763, 'annotation'] = ast.literal_eval('[[\"35 year old\"]]')\n",
    "train.loc[4763, 'location'] = ast.literal_eval('[[\"5 16\"]]')\n",
    "\n",
    "train.loc[4782, 'annotation'] = ast.literal_eval('[[\"darker brown stools\"]]')\n",
    "train.loc[4782, 'location'] = ast.literal_eval('[[\"175 194\"]]')\n",
    "\n",
    "train.loc[4908, 'annotation'] = ast.literal_eval('[[\"uncle with peptic ulcer\"]]')\n",
    "train.loc[4908, 'location'] = ast.literal_eval('[[\"700 723\"]]')\n",
    "\n",
    "train.loc[6016, 'annotation'] = ast.literal_eval('[[\"difficulty falling asleep\"]]')\n",
    "train.loc[6016, 'location'] = ast.literal_eval('[[\"225 250\"]]')\n",
    "\n",
    "train.loc[6192, 'annotation'] = ast.literal_eval('[[\"helps to take care of aging mother and in-laws\"]]')\n",
    "train.loc[6192, 'location'] = ast.literal_eval('[[\"197 218;236 260\"]]')\n",
    "\n",
    "train.loc[6380, 'annotation'] = ast.literal_eval('[[\"No hair changes\"], [\"No skin changes\"], [\"No GI changes\"], [\"No palpitations\"], [\"No excessive sweating\"]]')\n",
    "train.loc[6380, 'location'] = ast.literal_eval('[[\"480 482;507 519\"], [\"480 482;499 503;512 519\"], [\"480 482;521 531\"], [\"480 482;533 545\"], [\"480 482;564 582\"]]')\n",
    "\n",
    "train.loc[6562, 'annotation'] = ast.literal_eval('[[\"stressed due to taking care of her mother\"], [\"stressed due to taking care of husbands parents\"]]')\n",
    "train.loc[6562, 'location'] = ast.literal_eval('[[\"290 320;327 337\"], [\"290 320;342 358\"]]')\n",
    "\n",
    "train.loc[6862, 'annotation'] = ast.literal_eval('[[\"stressor taking care of many sick family members\"]]')\n",
    "train.loc[6862, 'location'] = ast.literal_eval('[[\"288 296;324 363\"]]')\n",
    "\n",
    "train.loc[7022, 'annotation'] = ast.literal_eval('[[\"heart started racing and felt numbness for the 1st time in her finger tips\"]]')\n",
    "train.loc[7022, 'location'] = ast.literal_eval('[[\"108 182\"]]')\n",
    "\n",
    "train.loc[7422, 'annotation'] = ast.literal_eval('[[\"first started 5 yrs\"]]')\n",
    "train.loc[7422, 'location'] = ast.literal_eval('[[\"102 121\"]]')\n",
    "\n",
    "train.loc[8876, 'annotation'] = ast.literal_eval('[[\"No shortness of breath\"]]')\n",
    "train.loc[8876, 'location'] = ast.literal_eval('[[\"481 483;533 552\"]]')\n",
    "\n",
    "train.loc[9027, 'annotation'] = ast.literal_eval('[[\"recent URI\"], [\"nasal stuffines, rhinorrhea, for 3-4 days\"]]')\n",
    "train.loc[9027, 'location'] = ast.literal_eval('[[\"92 102\"], [\"123 164\"]]')\n",
    "\n",
    "train.loc[9938, 'annotation'] = ast.literal_eval('[[\"irregularity with her cycles\"], [\"heavier bleeding\"], [\"changes her pad every couple hours\"]]')\n",
    "train.loc[9938, 'location'] = ast.literal_eval('[[\"89 117\"], [\"122 138\"], [\"368 402\"]]')\n",
    "\n",
    "train.loc[9973, 'annotation'] = ast.literal_eval('[[\"gaining 10-15 lbs\"]]')\n",
    "train.loc[9973, 'location'] = ast.literal_eval('[[\"344 361\"]]')\n",
    "\n",
    "train.loc[10513, 'annotation'] = ast.literal_eval('[[\"weight gain\"], [\"gain of 10-16lbs\"]]')\n",
    "train.loc[10513, 'location'] = ast.literal_eval('[[\"600 611\"], [\"607 623\"]]')\n",
    "\n",
    "train.loc[11551, 'annotation'] = ast.literal_eval('[[\"seeing her son knows are not real\"]]')\n",
    "train.loc[11551, 'location'] = ast.literal_eval('[[\"386 400;443 461\"]]')\n",
    "\n",
    "train.loc[11677, 'annotation'] = ast.literal_eval('[[\"saw him once in the kitchen after he died\"]]')\n",
    "train.loc[11677, 'location'] = ast.literal_eval('[[\"160 201\"]]')\n",
    "\n",
    "train.loc[12124, 'annotation'] = ast.literal_eval('[[\"tried Ambien but it didnt work\"]]')\n",
    "train.loc[12124, 'location'] = ast.literal_eval('[[\"325 337;349 366\"]]')\n",
    "\n",
    "train.loc[12279, 'annotation'] = ast.literal_eval('[[\"heard what she described as a party later than evening these things did not actually happen\"]]')\n",
    "train.loc[12279, 'location'] = ast.literal_eval('[[\"405 459;488 524\"]]')\n",
    "\n",
    "train.loc[12289, 'annotation'] = ast.literal_eval('[[\"experienced seeing her son at the kitchen table these things did not actually happen\"]]')\n",
    "train.loc[12289, 'location'] = ast.literal_eval('[[\"353 400;488 524\"]]')\n",
    "\n",
    "train.loc[13238, 'annotation'] = ast.literal_eval('[[\"SCRACHY THROAT\"], [\"RUNNY NOSE\"]]')\n",
    "train.loc[13238, 'location'] = ast.literal_eval('[[\"293 307\"], [\"321 331\"]]')\n",
    "\n",
    "train.loc[13297, 'annotation'] = ast.literal_eval('[[\"without improvement when taking tylenol\"], [\"without improvement when taking ibuprofen\"]]')\n",
    "train.loc[13297, 'location'] = ast.literal_eval('[[\"182 221\"], [\"182 213;225 234\"]]')\n",
    "\n",
    "train.loc[13299, 'annotation'] = ast.literal_eval('[[\"yesterday\"], [\"yesterday\"]]')\n",
    "train.loc[13299, 'location'] = ast.literal_eval('[[\"79 88\"], [\"409 418\"]]')\n",
    "\n",
    "train.loc[13845, 'annotation'] = ast.literal_eval('[[\"headache global\"], [\"headache throughout her head\"]]')\n",
    "train.loc[13845, 'location'] = ast.literal_eval('[[\"86 94;230 236\"], [\"86 94;237 256\"]]')\n",
    "\n",
    "train.loc[14083, 'annotation'] = ast.literal_eval('[[\"headache generalized in her head\"]]')\n",
    "train.loc[14083, 'location'] = ast.literal_eval('[[\"56 64;156 179\"]]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:  When they have things like `[\"a b;c d\"]` they are splicing together text from different parts of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    8185\n",
      "0    4399\n",
      "2    1292\n",
      "3     287\n",
      "4      99\n",
      "5      27\n",
      "6       9\n",
      "7       1\n",
      "8       1\n",
      "Name: annotation_length, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train['annotation_length'] = train['annotation'].apply(len)\n",
    "print(train['annotation_length'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This splits the training set into folds. Useful for actual training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold\n",
      "0    2860\n",
      "1    2860\n",
      "2    2860\n",
      "3    2860\n",
      "4    2860\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "Fold = GroupKFold(n_splits=CFG.n_fold)\n",
    "groups = train['pn_num'].values\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(train, train['location'], groups)):\n",
    "    train.loc[val_index, 'fold'] = int(n)\n",
    "train['fold'] = train['fold'].astype(int)\n",
    "print(train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n",
    "tokenizer.save_pretrained('tokenizer/')\n",
    "CFG.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42146/42146 [00:29<00:00, 1450.55it/s]\n",
      "pn_history max(lengths): 433\n",
      "100%|██████████| 143/143 [00:00<00:00, 5991.02it/s]\n",
      "feature_text max(lengths): 30\n",
      "max_len: 466\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Define max_len\n",
    "# ====================================================\n",
    "for text_col in ['pn_history']:\n",
    "    pn_history_lengths = []\n",
    "    tk0 = tqdm(patient_notes[text_col].fillna(\"\").values, total=len(patient_notes))\n",
    "    for text in tk0:\n",
    "        length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "        pn_history_lengths.append(length)\n",
    "    LOGGER.info(f'{text_col} max(lengths): {max(pn_history_lengths)}')\n",
    "\n",
    "for text_col in ['feature_text']:\n",
    "    features_lengths = []\n",
    "    tk0 = tqdm(features[text_col].fillna(\"\").values, total=len(features))\n",
    "    for text in tk0:\n",
    "        length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "        features_lengths.append(length)\n",
    "    LOGGER.info(f'{text_col} max(lengths): {max(features_lengths)}')\n",
    "\n",
    "CFG.max_len = max(pn_history_lengths) + max(features_lengths) + 3 # cls & sep & sep\n",
    "LOGGER.info(f\"max_len: {CFG.max_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/42146 [00:00<?, ?it/s]\n",
      "pn_history max(lengths): 217\n"
     ]
    }
   ],
   "source": [
    "for text_col in ['pn_history']:\n",
    "    pn_history_lengths = []\n",
    "    tk0 = tqdm(patient_notes[text_col].fillna(\"\").values, total=len(patient_notes))\n",
    "    for text in tk0:\n",
    "        token = tokenizer(text, add_special_tokens=False)\n",
    "        length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "        pn_history_lengths.append(length)\n",
    "        break\n",
    "    LOGGER.info(f'{text_col} max(lengths): {max(pn_history_lengths)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1360, 12, 180, 12, 279, 2943, 6, 34, 283, 7, 5, 1294, 474, 8474, 13689, 9, 1144, 29839, 4, 427, 4, 2986, 18, 985, 34, 576, 14580, 7132, 13, 10, 750, 6, 2166, 9027, 6, 8, 1416, 50121, 50118, 12, 1610, 3494, 132, 12, 246, 377, 536, 6, 29, 37710, 6, 8007, 22141, 1342, 13, 132, 360, 1640, 25179, 155, 12, 306, 5251, 238, 605, 994, 4226, 6, 13424, 12, 337, 9525, 73, 7165, 6472, 50121, 50118, 12, 38838, 19, 11734, 22423, 15, 1931, 39565, 8, 1079, 6, 620, 10887, 66, 59, 334, 50121, 50118, 12, 13685, 10668, 2653, 101, 39, 1144, 16, 9755, 66, 9, 39, 7050, 50121, 50118, 12, 3985, 35, 3898, 918, 7050, 2400, 6, 7180, 8258, 4765, 354, 6, 43192, 872, 6, 611, 5622, 6, 506, 6294, 6, 282, 17498, 102, 6, 705, 1075, 2838, 6, 9700, 337, 4803, 242, 424, 50121, 50118, 12, 1685, 298, 35, 13424, 6, 4567, 29, 4832, 7292, 1168, 36, 7761, 10, 1441, 238, 282, 330, 6106, 50121, 50118, 12, 506, 298, 35, 14891, 56, 10931, 682, 6, 19456, 34, 33670, 385, 329, 50121, 50118, 12, 1193, 35, 13424, 12, 9426, 21783, 6, 3916, 1023, 30585, 195, 12, 401, 377, 536, 6, 246, 16328, 15, 5, 983, 6, 2613, 23, 334, 50121, 50118, 12, 1193, 35, 2362, 49008], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.deberta.tokenization_deberta_fast.DebertaTokenizerFast"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(CFG.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method PreTrainedTokenizerBase.__call__ of PreTrainedTokenizerFast(name_or_path='microsoft/deberta-base', vocab_size=50265, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'sep_token': AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'cls_token': AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'mask_token': AddedToken(\"[MASK]\", rstrip=False, lstrip=True, single_word=False, normalized=True)})>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CFG.tokenizer.__call__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Dataset\n",
    "# ====================================================\n",
    "def prepare_input(cfg, text, feature_text):\n",
    "    inputs = cfg.tokenizer(text, feature_text, \n",
    "                           add_special_tokens=True,\n",
    "                           max_length=CFG.max_len,\n",
    "                           padding=\"max_length\",\n",
    "                           return_offsets_mapping=False)\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BatchEncoding\n",
    "def create_label(cfg, text, annotation_length, location_list):\n",
    "    encoded:BatchEncoding = cfg.tokenizer(text,\n",
    "                            add_special_tokens=True,\n",
    "                            max_length=CFG.max_len,\n",
    "                            padding=\"max_length\",\n",
    "                            return_offsets_mapping=True)\n",
    "    offset_mapping:List[Tuple[int, int]] = encoded['offset_mapping'] # Maps back to the original text (e.g. 'Hello' -> 'He', 'llo', [(0, 2), (2, 5)])\n",
    "    ignore_idxes:np.ndarray = np.where(np.array(encoded.sequence_ids()) != 0)[0]  # Indexes of the special tokens, see https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/tokenizer#transformers.BatchEncoding\n",
    "    label = np.zeros(len(offset_mapping)) # Create a label for each token\n",
    "    label[ignore_idxes] = -1 # Special tokens are given label -1 by default\n",
    "    if annotation_length != 0:\n",
    "        for location in location_list:\n",
    "            for loc in [s.split() for s in location.split(';')]:\n",
    "                start_idx = -1\n",
    "                end_idx = -1\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                for idx in range(len(offset_mapping)):\n",
    "                    if (start_idx == -1) & (start < offset_mapping[idx][0]):\n",
    "                        start_idx = idx - 1 # Give start the first token before the first token contained in [start, end]\n",
    "                    if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n",
    "                        end_idx = idx + 1 # Give end the first token after the last token contained in [start, end]\n",
    "                if start_idx == -1:\n",
    "                    start_idx = end_idx\n",
    "                if (start_idx != -1) & (end_idx != -1):\n",
    "                    label[start_idx:end_idx] = 1 # Give a token label of 1 if the idxs it corresponds to are within (start, end)\n",
    "    return torch.tensor(label, dtype=torch.float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.feature_texts = df['feature_text'].values\n",
    "        self.pn_historys = df['pn_history'].values\n",
    "        self.annotation_lengths = df['annotation_length'].values\n",
    "        self.locations = df['location'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feature_texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_input(self.cfg, \n",
    "                               self.pn_historys[item], \n",
    "                               self.feature_texts[item])\n",
    "        label = create_label(self.cfg, \n",
    "                             self.pn_historys[item], \n",
    "                             self.annotation_lengths[item], \n",
    "                             self.locations[item])\n",
    "        return inputs, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Model\n",
    "# ====================================================\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n",
    "        else:\n",
    "            self.model = AutoModel(self.config)\n",
    "        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 1)\n",
    "        self._init_weights(self.fc)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            print('Overwriting existing embeddings!')\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        \n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_states = outputs[0]\n",
    "        return last_hidden_states\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        feature = self.feature(inputs)\n",
    "        output = self.fc(self.fc_dropout(feature))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TrainDataset(CFG, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(data,\n",
    "                              batch_size=5,\n",
    "                              shuffle=True,\n",
    "                              num_workers=1, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = CustomModel(CFG, config_path=None, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 466])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input = next(iter(train_loader))[0]\n",
    "test_input['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 466, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(test_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Helper functions\n",
    "# ====================================================\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex) # scaled gradeint up to avoid gradient underflow for some types\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    for step, (inputs, labels) in enumerate(train_loader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "            y_preds = model(inputs)\n",
    "        loss = criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1).mean()\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "            if CFG.batch_scheduler:\n",
    "                scheduler.step()\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.8f}  '\n",
    "                  .format(epoch+1, step, len(train_loader), \n",
    "                          remain=timeSince(start, float(step+1)/len(train_loader)),\n",
    "                          loss=losses,\n",
    "                          grad_norm=grad_norm,\n",
    "                          lr=scheduler.get_lr()[0]))\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    start = end = time.time()\n",
    "    for step, (inputs, labels) in enumerate(valid_loader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        loss = criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1).mean()\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(y_preds.sigmoid().to('cpu').numpy())\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(step, len(valid_loader),\n",
    "                          loss=losses,\n",
    "                          remain=timeSince(start, float(step+1)/len(valid_loader))))\n",
    "    predictions = np.concatenate(preds)\n",
    "    return losses.avg, predictions\n",
    "\n",
    "\n",
    "def inference_fn(test_loader, model, device):\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    tk0 = tqdm(test_loader, total=len(test_loader))\n",
    "    for inputs in tk0:\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        preds.append(y_preds.sigmoid().to('cpu').numpy())\n",
    "    predictions = np.concatenate(preds)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# train loop\n",
    "# ====================================================\n",
    "def train_loop(folds, fold):\n",
    "    \n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    train_folds = folds[folds['fold'] != fold].reset_index(drop=True)\n",
    "    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n",
    "    valid_texts = valid_folds['pn_history'].values\n",
    "    valid_labels = create_labels_for_scoring(valid_folds)\n",
    "    \n",
    "    train_dataset = TrainDataset(CFG, train_folds)\n",
    "    valid_dataset = TrainDataset(CFG, valid_folds)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "    model = CustomModel(CFG, config_path=None, pretrained=True)\n",
    "    torch.save(model.config, 'config.pth')\n",
    "    model.to(device)\n",
    "    \n",
    "    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "             'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "\n",
    "    optimizer_parameters = get_optimizer_params(model,\n",
    "                                                encoder_lr=CFG.encoder_lr, \n",
    "                                                decoder_lr=CFG.decoder_lr,\n",
    "                                                weight_decay=CFG.weight_decay)\n",
    "    optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)\n",
    "    \n",
    "    # ====================================================\n",
    "    # scheduler\n",
    "    # ====================================================\n",
    "    def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "        if cfg.scheduler=='linear':\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "            )\n",
    "        elif cfg.scheduler=='cosine':\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n",
    "            )\n",
    "        return scheduler\n",
    "    \n",
    "    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n",
    "    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    \n",
    "    best_score = 0.\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # train\n",
    "        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n",
    "\n",
    "        # eval\n",
    "        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, device)\n",
    "        predictions = predictions.reshape((len(valid_folds), CFG.max_len))\n",
    "        \n",
    "        # scoring\n",
    "        char_probs = get_char_probs(valid_texts, predictions, CFG.tokenizer)\n",
    "        results = get_results(char_probs, th=0.5)\n",
    "        preds = get_predictions(results)\n",
    "        score = get_score(valid_labels, preds)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n",
    "        \n",
    "        if best_score < score:\n",
    "            best_score = score\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'predictions': predictions},\n",
    "                        f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\")\n",
    "\n",
    "    predictions = torch.load(f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\", \n",
    "                             map_location=torch.device('cpu'))['predictions']\n",
    "    valid_folds[[i for i in range(CFG.max_len)]] = predictions\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return valid_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_result(oof_df):\n",
    "    labels = create_labels_for_scoring(oof_df)\n",
    "    predictions = oof_df[[i for i in range(CFG.max_len)]].values\n",
    "    char_probs = get_char_probs(oof_df['pn_history'].values, predictions, CFG.tokenizer)\n",
    "    results = get_results(char_probs, th=0.5)\n",
    "    preds = get_predictions(results)\n",
    "    score = get_score(labels, preds)\n",
    "    LOGGER.info(f'Score: {score:<.4f}')\n",
    "\n",
    "if False:\n",
    "    oof_df = pd.DataFrame()\n",
    "    for fold in range(CFG.n_fold):\n",
    "        if fold in CFG.trn_fold:\n",
    "            _oof_df = train_loop(train, fold)\n",
    "            oof_df = pd.concat([oof_df, _oof_df])\n",
    "            LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "            get_result(_oof_df)\n",
    "    oof_df = oof_df.reset_index(drop=True)\n",
    "    LOGGER.info(f\"========== CV ==========\")\n",
    "    get_result(oof_df)\n",
    "    oof_df.to_pickle('oof_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ ] What do the train and test dataframe inputs to the model look like?\n",
    "\n",
    "[ ] What to the predictions of the model look like?\n",
    "\n",
    "[ ] Are we overwriting the existing embeddingso f the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What do the train and test dataframe inputs to the model look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 0\n",
    "\n",
    "train_folds = train[train['fold'] != fold].reset_index(drop=True)\n",
    "valid_folds = train[train['fold'] == fold].reset_index(drop=True)\n",
    "valid_texts = valid_folds['pn_history'].values\n",
    "valid_labels = create_labels_for_scoring(valid_folds)\n",
    "\n",
    "train_dataset = TrainDataset(CFG, train_folds)\n",
    "valid_dataset = TrainDataset(CFG, valid_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pandas.core.frame.DataFrame, pandas.core.frame.DataFrame)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_folds), type(valid_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[622, 631]],\n",
       " [[633, 652]],\n",
       " [],\n",
       " [[76, 84], [171, 180]],\n",
       " [[254, 270]],\n",
       " [],\n",
       " [[389, 396]],\n",
       " [[284, 303]],\n",
       " [],\n",
       " [[85, 99], [126, 138], [126, 131], [143, 151]]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding, label = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(pd.DataFrame(index=np.arange(len(label)), data=label.numpy().reshape(-1), columns=['']).loc[160:190], cmap='viridis', vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the x-part is a `BatchEncoding` of the `pn_history` followed by the `feature_text`. The y-part is a token-level encoding of the `pn_history` with -1 for special tokens, 0 for non-target tokens, and 1 for tarket tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 42891, 2, 8331, 2], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CFG.tokenizer('hello', 'world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[SEP]'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CFG.tokenizer.decode(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 42891, 232, 2], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CFG.tokenizer('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.deberta.tokenization_deberta_fast.DebertaTokenizerFast"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(CFG.tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The thing we are seeing with the Ġworld seems to be a `prefix space` and detailed [here](https://discuss.huggingface.co/t/bpe-tokenizers-and-spaces-before-words/475). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Are we overwriting the existing embeddings of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = CustomModel(CFG, config_path=None, pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope! We were just applying weight initialization to the fully connected layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Data Cleaning Based on Theo Veil's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make sure that all the data is in a consistent format, but I don't think as part of this step of data cleaning we should apply tokenization as that is model-specific. Abstractly, the goal is going to be to have the following for each labeled instance:\n",
    "\n",
    "- The pn_history as a string with any mistakes that would throw off tokenizers removed\n",
    "- The feature text as a string with any mistakes that would throw off tokenizers removed\n",
    "- The annotations as a list of (S, E) pairs which have the property that `pn_history[S:E]` is relevant to the feature text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project.data.data_loaders import get_features, get_patient_notes, get_train\n",
    "train, features, patient_notes = get_train(), get_features(), get_patient_notes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\" # This feature is incorrectly coded as 'Last-Pap-smear-I-year-ago'\n",
    "train = train.merge(features, on=['feature_num', 'case_num'], how='left').merge(patient_notes, on=['pn_num', 'case_num'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrections taken from https://www.kaggle.com/code/naveace/nbme-deberta-base-baseline-train/edit\n",
    "for idx, correction_dict in json.load(open('annotation_corrections.json', 'r')).items():\n",
    "    idx = int(idx)\n",
    "    train.loc[idx, 'annotation'] = correction_dict['correct annotation']\n",
    "    train.loc[idx, 'location'] = correction_dict['correct location']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In location, every entry is a list. For all strings in the list, they are either of the form `'s e'` or `'s e;s e;...'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['annotation'] = train['annotation'].map(eval)\n",
    "train['location'] = train['location'].map(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "def get_start_and_end_indices(locations: str) -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Maps strings of the form '\\d+ \\d+(;\\d+ \\d+)*' to a list of tuples of the form (start, end).\n",
    "    E.g. '1 2' -> [(1, 2)]\n",
    "         '1 2;3 4' -> [(1, 2), (3, 4)]\n",
    "         '' -> []\n",
    "    \"\"\"\n",
    "    assert(isinstance(locations, str))\n",
    "    if locations == '':\n",
    "        return []\n",
    "    if not ';' in locations:\n",
    "        s, e = locations.split()\n",
    "        return [(int(s), int(e))]\n",
    "    return list(map(lambda s: get_start_and_end_indices(s)[0], locations.split(';')))\n",
    "def convert_to_idx_tuple_pairs(location_list: List[str]) -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Maps a list of strings of the form '\\d+ \\d+(;\\d+ \\d+)*' to a list of tuples of the form (start, end).\n",
    "    E.g. ['1 2', '3 4'] -> [(1, 2), (3, 4)]\n",
    "    \"\"\"\n",
    "    return list(reduce(lambda l1, l2: l1 + l2, map(get_start_and_end_indices, location_list), []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['location_idx_tuple_pairs'] = train['location'].map(convert_to_idx_tuple_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_correctness(labels):\n",
    "    \"\"\"\n",
    "    Asserts correct types for labels\n",
    "    \"\"\"\n",
    "    for idx, element in enumerate(labels):\n",
    "        try:\n",
    "            assert(isinstance(element, list))\n",
    "            for obj in element:\n",
    "                assert(isinstance(obj, tuple))\n",
    "                assert(len(obj) == 2)\n",
    "                assert(isinstance(obj[0], int))\n",
    "                assert(isinstance(obj[1], int))\n",
    "        except AssertionError:\n",
    "            print(f'Failed assertion for element {idx}: {element}')\n",
    "        \n",
    "assert_correctness(train['location_idx_tuple_pairs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping it all up into one data cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "def get_start_and_end_indices(locations: str) -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Maps strings of the form '\\d+ \\d+(;\\d+ \\d+)*' to a list of tuples of the form (start, end).\n",
    "    E.g. '1 2' -> [(1, 2)]\n",
    "         '1 2;3 4' -> [(1, 2), (3, 4)]\n",
    "         '' -> []\n",
    "    \"\"\"\n",
    "    assert(isinstance(locations, str))\n",
    "    if locations == '':\n",
    "        return []\n",
    "    if not ';' in locations:\n",
    "        s, e = locations.split()\n",
    "        return [(int(s), int(e))]\n",
    "    return list(map(lambda s: get_start_and_end_indices(s)[0], locations.split(';')))\n",
    "def convert_to_idx_tuple_pairs(location_list: List[str]) -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Maps a list of strings of the form '\\d+ \\d+(;\\d+ \\d+)*' to a list of tuples of the form (start, end).\n",
    "    E.g. ['1 2', '3 4'] -> [(1, 2), (3, 4)]\n",
    "    \"\"\"\n",
    "    return list(reduce(lambda l1, l2: l1 + l2, map(get_start_and_end_indices, location_list), []))\n",
    "def assert_correctness(labels) -> None:\n",
    "    \"\"\"\n",
    "    Asserts correct types for labels\n",
    "    \"\"\"\n",
    "    for idx, element in enumerate(labels):\n",
    "        try:\n",
    "            assert(isinstance(element, list))\n",
    "            for obj in element:\n",
    "                assert(isinstance(obj, tuple))\n",
    "                assert(len(obj) == 2)\n",
    "                assert(isinstance(obj[0], int))\n",
    "                assert(isinstance(obj[1], int))\n",
    "        except AssertionError:\n",
    "            print(f'Failed assertion for element {idx}: {element}')\n",
    "\n",
    "def get_clean_train_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Produces a cleaned version of the train dataframe with the following:\n",
    "    id: a unique id for the train instance (as in Kaggle)\n",
    "    case_num: unique id for case (as in Kaggle)\n",
    "    pn_num: unique id for patient note (as in Kaggle)\n",
    "    feature_num: unique id for patient note (as in kaggle)\n",
    "    annotation: a List[str] of the text in patient note pn_num that corresponds to feature pn_num. \n",
    "        Each element is not necessarily contiguous but is human-readable. Some elements may splice together parts of patient note\n",
    "    location_raw: the raw location (as in Kaggle)\n",
    "    feature_text: the feature text for the feature feature_num (as in Kaggle)\n",
    "    pn_history: the patient note (as in Kaggle)\n",
    "    location: a List[Tuple[start: int, end: int]] of the ranges of characters in the patient note that corresponds to the feature (positives)\n",
    "    \"\"\"\n",
    "    train, features, patient_notes = get_train(), get_features(), get_patient_notes()\n",
    "    features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\" # This feature is incorrectly coded as 'Last-Pap-smear-I-year-ago'\n",
    "    train = train.merge(features, on=['feature_num', 'case_num'], how='left').merge(patient_notes, on=['pn_num', 'case_num'], how='left')\n",
    "    # Corrections taken from https://www.kaggle.com/code/naveace/nbme-deberta-base-baseline-train/edit\n",
    "    for idx, correction_dict in json.load(open('annotation_corrections.json', 'r')).items():\n",
    "        idx = int(idx)\n",
    "        train.loc[idx, 'annotation'] = correction_dict['correct annotation']\n",
    "        train.loc[idx, 'location'] = correction_dict['correct location']\n",
    "    # Creating cleaned `location` column\n",
    "    train['annotation'] = train['annotation'].map(eval)\n",
    "    train['location'] = train['location'].map(eval)\n",
    "    train['location_idx_tuple_pairs'] = train['location'].map(convert_to_idx_tuple_pairs)\n",
    "    assert_correctness(train['location_idx_tuple_pairs'])\n",
    "    train = train.rename(columns={'location': 'location_raw', 'location_idx_tuple_pairs': 'location'})\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reference only, this is the code use to make cleaning script\n",
    "annotation_correction_code = \\\n",
    "\"\"\"\n",
    "# incorrect annotation\n",
    "train.loc[338, 'annotation'] = ast.literal_eval('[[\"father heart attack\"]]')\n",
    "train.loc[338, 'location'] = ast.literal_eval('[[\"764 783\"]]')\n",
    "\n",
    "train.loc[621, 'annotation'] = ast.literal_eval('[[\"for the last 2-3 months\"]]')\n",
    "train.loc[621, 'location'] = ast.literal_eval('[[\"77 100\"]]')\n",
    "\n",
    "train.loc[655, 'annotation'] = ast.literal_eval('[[\"no heat intolerance\"], [\"no cold intolerance\"]]')\n",
    "train.loc[655, 'location'] = ast.literal_eval('[[\"285 292;301 312\"], [\"285 287;296 312\"]]')\n",
    "\n",
    "train.loc[1262, 'annotation'] = ast.literal_eval('[[\"mother thyroid problem\"]]')\n",
    "train.loc[1262, 'location'] = ast.literal_eval('[[\"551 557;565 580\"]]')\n",
    "\n",
    "train.loc[1265, 'annotation'] = ast.literal_eval('[[\\'felt like he was going to \"pass out\"\\']]')\n",
    "train.loc[1265, 'location'] = ast.literal_eval('[[\"131 135;181 212\"]]')\n",
    "\n",
    "train.loc[1396, 'annotation'] = ast.literal_eval('[[\"stool , with no blood\"]]')\n",
    "train.loc[1396, 'location'] = ast.literal_eval('[[\"259 280\"]]')\n",
    "\n",
    "train.loc[1591, 'annotation'] = ast.literal_eval('[[\"diarrhoe non blooody\"]]')\n",
    "train.loc[1591, 'location'] = ast.literal_eval('[[\"176 184;201 212\"]]')\n",
    "\n",
    "train.loc[1615, 'annotation'] = ast.literal_eval('[[\"diarrhea for last 2-3 days\"]]')\n",
    "train.loc[1615, 'location'] = ast.literal_eval('[[\"249 257;271 288\"]]')\n",
    "\n",
    "train.loc[1664, 'annotation'] = ast.literal_eval('[[\"no vaginal discharge\"]]')\n",
    "train.loc[1664, 'location'] = ast.literal_eval('[[\"822 824;907 924\"]]')\n",
    "\n",
    "train.loc[1714, 'annotation'] = ast.literal_eval('[[\"started about 8-10 hours ago\"]]')\n",
    "train.loc[1714, 'location'] = ast.literal_eval('[[\"101 129\"]]')\n",
    "\n",
    "train.loc[1929, 'annotation'] = ast.literal_eval('[[\"no blood in the stool\"]]')\n",
    "train.loc[1929, 'location'] = ast.literal_eval('[[\"531 539;549 561\"]]')\n",
    "\n",
    "train.loc[2134, 'annotation'] = ast.literal_eval('[[\"last sexually active 9 months ago\"]]')\n",
    "train.loc[2134, 'location'] = ast.literal_eval('[[\"540 560;581 593\"]]')\n",
    "\n",
    "train.loc[2191, 'annotation'] = ast.literal_eval('[[\"right lower quadrant pain\"]]')\n",
    "train.loc[2191, 'location'] = ast.literal_eval('[[\"32 57\"]]')\n",
    "\n",
    "train.loc[2553, 'annotation'] = ast.literal_eval('[[\"diarrhoea no blood\"]]')\n",
    "train.loc[2553, 'location'] = ast.literal_eval('[[\"308 317;376 384\"]]')\n",
    "\n",
    "train.loc[3124, 'annotation'] = ast.literal_eval('[[\"sweating\"]]')\n",
    "train.loc[3124, 'location'] = ast.literal_eval('[[\"549 557\"]]')\n",
    "\n",
    "train.loc[3858, 'annotation'] = ast.literal_eval('[[\"previously as regular\"], [\"previously eveyr 28-29 days\"], [\"previously lasting 5 days\"], [\"previously regular flow\"]]')\n",
    "train.loc[3858, 'location'] = ast.literal_eval('[[\"102 123\"], [\"102 112;125 141\"], [\"102 112;143 157\"], [\"102 112;159 171\"]]')\n",
    "\n",
    "train.loc[4373, 'annotation'] = ast.literal_eval('[[\"for 2 months\"]]')\n",
    "train.loc[4373, 'location'] = ast.literal_eval('[[\"33 45\"]]')\n",
    "\n",
    "train.loc[4763, 'annotation'] = ast.literal_eval('[[\"35 year old\"]]')\n",
    "train.loc[4763, 'location'] = ast.literal_eval('[[\"5 16\"]]')\n",
    "\n",
    "train.loc[4782, 'annotation'] = ast.literal_eval('[[\"darker brown stools\"]]')\n",
    "train.loc[4782, 'location'] = ast.literal_eval('[[\"175 194\"]]')\n",
    "\n",
    "train.loc[4908, 'annotation'] = ast.literal_eval('[[\"uncle with peptic ulcer\"]]')\n",
    "train.loc[4908, 'location'] = ast.literal_eval('[[\"700 723\"]]')\n",
    "\n",
    "train.loc[6016, 'annotation'] = ast.literal_eval('[[\"difficulty falling asleep\"]]')\n",
    "train.loc[6016, 'location'] = ast.literal_eval('[[\"225 250\"]]')\n",
    "\n",
    "train.loc[6192, 'annotation'] = ast.literal_eval('[[\"helps to take care of aging mother and in-laws\"]]')\n",
    "train.loc[6192, 'location'] = ast.literal_eval('[[\"197 218;236 260\"]]')\n",
    "\n",
    "train.loc[6380, 'annotation'] = ast.literal_eval('[[\"No hair changes\"], [\"No skin changes\"], [\"No GI changes\"], [\"No palpitations\"], [\"No excessive sweating\"]]')\n",
    "train.loc[6380, 'location'] = ast.literal_eval('[[\"480 482;507 519\"], [\"480 482;499 503;512 519\"], [\"480 482;521 531\"], [\"480 482;533 545\"], [\"480 482;564 582\"]]')\n",
    "\n",
    "train.loc[6562, 'annotation'] = ast.literal_eval('[[\"stressed due to taking care of her mother\"], [\"stressed due to taking care of husbands parents\"]]')\n",
    "train.loc[6562, 'location'] = ast.literal_eval('[[\"290 320;327 337\"], [\"290 320;342 358\"]]')\n",
    "\n",
    "train.loc[6862, 'annotation'] = ast.literal_eval('[[\"stressor taking care of many sick family members\"]]')\n",
    "train.loc[6862, 'location'] = ast.literal_eval('[[\"288 296;324 363\"]]')\n",
    "\n",
    "train.loc[7022, 'annotation'] = ast.literal_eval('[[\"heart started racing and felt numbness for the 1st time in her finger tips\"]]')\n",
    "train.loc[7022, 'location'] = ast.literal_eval('[[\"108 182\"]]')\n",
    "\n",
    "train.loc[7422, 'annotation'] = ast.literal_eval('[[\"first started 5 yrs\"]]')\n",
    "train.loc[7422, 'location'] = ast.literal_eval('[[\"102 121\"]]')\n",
    "\n",
    "train.loc[8876, 'annotation'] = ast.literal_eval('[[\"No shortness of breath\"]]')\n",
    "train.loc[8876, 'location'] = ast.literal_eval('[[\"481 483;533 552\"]]')\n",
    "\n",
    "train.loc[9027, 'annotation'] = ast.literal_eval('[[\"recent URI\"], [\"nasal stuffines, rhinorrhea, for 3-4 days\"]]')\n",
    "train.loc[9027, 'location'] = ast.literal_eval('[[\"92 102\"], [\"123 164\"]]')\n",
    "\n",
    "train.loc[9938, 'annotation'] = ast.literal_eval('[[\"irregularity with her cycles\"], [\"heavier bleeding\"], [\"changes her pad every couple hours\"]]')\n",
    "train.loc[9938, 'location'] = ast.literal_eval('[[\"89 117\"], [\"122 138\"], [\"368 402\"]]')\n",
    "\n",
    "train.loc[9973, 'annotation'] = ast.literal_eval('[[\"gaining 10-15 lbs\"]]')\n",
    "train.loc[9973, 'location'] = ast.literal_eval('[[\"344 361\"]]')\n",
    "\n",
    "train.loc[10513, 'annotation'] = ast.literal_eval('[[\"weight gain\"], [\"gain of 10-16lbs\"]]')\n",
    "train.loc[10513, 'location'] = ast.literal_eval('[[\"600 611\"], [\"607 623\"]]')\n",
    "\n",
    "train.loc[11551, 'annotation'] = ast.literal_eval('[[\"seeing her son knows are not real\"]]')\n",
    "train.loc[11551, 'location'] = ast.literal_eval('[[\"386 400;443 461\"]]')\n",
    "\n",
    "train.loc[11677, 'annotation'] = ast.literal_eval('[[\"saw him once in the kitchen after he died\"]]')\n",
    "train.loc[11677, 'location'] = ast.literal_eval('[[\"160 201\"]]')\n",
    "\n",
    "train.loc[12124, 'annotation'] = ast.literal_eval('[[\"tried Ambien but it didnt work\"]]')\n",
    "train.loc[12124, 'location'] = ast.literal_eval('[[\"325 337;349 366\"]]')\n",
    "\n",
    "train.loc[12279, 'annotation'] = ast.literal_eval('[[\"heard what she described as a party later than evening these things did not actually happen\"]]')\n",
    "train.loc[12279, 'location'] = ast.literal_eval('[[\"405 459;488 524\"]]')\n",
    "\n",
    "train.loc[12289, 'annotation'] = ast.literal_eval('[[\"experienced seeing her son at the kitchen table these things did not actually happen\"]]')\n",
    "train.loc[12289, 'location'] = ast.literal_eval('[[\"353 400;488 524\"]]')\n",
    "\n",
    "train.loc[13238, 'annotation'] = ast.literal_eval('[[\"SCRACHY THROAT\"], [\"RUNNY NOSE\"]]')\n",
    "train.loc[13238, 'location'] = ast.literal_eval('[[\"293 307\"], [\"321 331\"]]')\n",
    "\n",
    "train.loc[13297, 'annotation'] = ast.literal_eval('[[\"without improvement when taking tylenol\"], [\"without improvement when taking ibuprofen\"]]')\n",
    "train.loc[13297, 'location'] = ast.literal_eval('[[\"182 221\"], [\"182 213;225 234\"]]')\n",
    "\n",
    "train.loc[13299, 'annotation'] = ast.literal_eval('[[\"yesterday\"], [\"yesterday\"]]')\n",
    "train.loc[13299, 'location'] = ast.literal_eval('[[\"79 88\"], [\"409 418\"]]')\n",
    "\n",
    "train.loc[13845, 'annotation'] = ast.literal_eval('[[\"headache global\"], [\"headache throughout her head\"]]')\n",
    "train.loc[13845, 'location'] = ast.literal_eval('[[\"86 94;230 236\"], [\"86 94;237 256\"]]')\n",
    "\n",
    "train.loc[14083, 'annotation'] = ast.literal_eval('[[\"headache generalized in her head\"]]')\n",
    "train.loc[14083, 'location'] = ast.literal_eval('[[\"56 64;156 179\"]]')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reference only, this is the cleaning script\n",
    "import re\n",
    "from typing import Dict\n",
    "corrections:Dict[str, Dict[str, str]] = {}\n",
    "for idx, correct_string in re.findall(r\"train.loc\\[([0-9]+), 'annotation'][ ]*=[ ]*ast.literal_eval\\('(.*)'\\)\", annotation_correction_code, re.MULTILINE):\n",
    "    result_list = eval(correct_string)  # He doubly nested corrections to match his format in create_labels_for_scoring\n",
    "    result_list = [l[0] for l in result_list]\n",
    "    corrections[idx] = {'correct annotation': str(result_list)} # Cast to string to be consistent with other data and avoid special cases\n",
    "for idx, correct_location in re.findall(r\"train.loc\\[([0-9]+), 'location'][ ]*=[ ]*ast.literal_eval\\('(.*)'\\)\", annotation_correction_code, re.MULTILINE):\n",
    "    result_list = eval(correct_location)\n",
    "    result_list = [l[0] for l in result_list]\n",
    "    corrections[idx].update({'correct location': str(result_list)})  # Cast to string to be consistent with other data and avoid special cases\n",
    "json.dump(corrections, open('annotation_corrections.json', 'w'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project.data.data_loaders import get_clean_train_data\n",
    "train = get_clean_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 14300 entries, 0 to 14299\n",
      "Data columns (total 9 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   id            14300 non-null  object\n",
      " 1   case_num      14300 non-null  int64 \n",
      " 2   pn_num        14300 non-null  int64 \n",
      " 3   feature_num   14300 non-null  int64 \n",
      " 4   annotation    14300 non-null  object\n",
      " 5   location_raw  14300 non-null  object\n",
      " 6   feature_text  14300 non-null  object\n",
      " 7   pn_history    14300 non-null  object\n",
      " 8   location      14300 non-null  object\n",
      "dtypes: int64(3), object(6)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Feature: Worse-with-deep-breath-OR-pleuritic\n",
      "***Patient note: 17y/o male c/o chest pain\n",
      "he woke up with the chest pain yesterday monining, it is a sharp pain that is every time when he brathes in. denies trauma. no radiation of pain. Took tylonol several times, helped a little. \n",
      "(-) sick contacts, cough. \n",
      "(+) muscle aches, \n",
      "NKDA. Exercise induced-asthma. Takes albuterol inhaler . \n",
      "Father: asthma, high cholesterol, Mother: no known diseases. \n",
      "Climber, lives with parents and sister and brother. \n",
      "(-) smoking. drinks beers on parties. \n",
      "\n",
      "***Identified locations:\n",
      "\t None\n",
      "***Feature: Irregular-flow-OR-Irregular-frequency-OR-Irregular-intervals\n",
      "***Patient note: The patient is a 44 year old female who presents with irregular periods. Her last period was 2 months ago. Over the past 3 years her periods have been \"unpredictable,\" with various amounts of bleeding and frequency of menstrual cycles. She also endorses vaginal dryness, discomfort with intercourse, hot flashes, and nausea and vomiting last week. She has an IUD x 4 years. She typically has mild cramps and breast tenderness with periods. Menarche was age 14, first pregnancy age 30, second pregnancy age 34. Both pregnancies uncomplicated. Patient denies HA, chest pain, SOA, abdominal pain. Denies dysuria, hematuria. Denies seeing blood in her stool. Denies unintentional wt loss, weight gain, dry skin. No breast discharge. \n",
      "\n",
      "PMH: HTN \n",
      "PSH: none \n",
      "Medication: HCTZ\n",
      "Allergies: none \n",
      "FH: unremarkable \n",
      "SH: no tobacco, ETOH rare, no drugs. Sexually active with husband only. Feels safe at home. IUD in place. \n",
      "ROS otherwise negative \n",
      "\n",
      "***Identified locations:\n",
      "\tvarious amounts of bleeding\n",
      "\tvarious\n",
      "\tfrequency\n",
      "***Feature: Diminished-energy-OR-feeling-drained\n",
      "***Patient note: 67yo woman presents with 3 weeks of difficulty falling asleep.  3 weeks ago her son was killed in an auto accident.  Pt says that it takes her a long time to fall asleep as shes thinking about her son, and she tosses and turns.  Pt has tried to take naps during the day, but has been unable to fall asleep.  Pt endorses a good support network including a loving and understanding husband.  Pt endorses increased appetite lately.  No change in weight.  Pt describes herself as sad, but not depressed.  She has diminished interest in hobbies, but is socializing.  Pt tried ambien to sleep but it didn't help.  Pt denies chest pain, SOB, fever, chills, nausea, vomiting, abdominal pain.  PMH sig for HTN treated with lisinopril and HCTZ, and pt is a 10yr breast CA survivor s/p lumpectomy.  Pt doesn't smoke, occassional EtOH use, no illicit drugs. Home BPs measurements have been good.\n",
      "***Identified locations:\n",
      "\t None\n",
      "***Feature: Son-died-3-weeks-ago\n",
      "***Patient note: 67 yo F presenting with 3 weeks of insomnia. Pt reports difficulty falling asleep, waking up early, and daytime somulance. Pt states her son passed away on Aug 31st of this year. She state feeling sad and missing him. She alos reports seeing him sitting at the breakfast table 3 days ago. She is aware that this was not really happening. She also mentioned hearing distant music and talking at neighbors house, but no one was there. Denies command voieces. \n",
      "\n",
      "Denies snoring, suicidal/homicidal thoughts, constipation, dry brittle skin, recent medication changes\n",
      "\n",
      "PMH: HTN, breast cancer s/p lumectomy, radiation, and chemo. remission for past 10 years\n",
      "PSH: lumectomy, lap appy\n",
      "Meds: HCTZ, lisinopril\n",
      "Allergies: NKDA\n",
      "FH: father with HLD, HTN died from storke. Mother with h/o depression\n",
      "Social: drinks 2-3 glasses of wine a week. Denies smoking, recreational drug use\n",
      "***Identified locations:\n",
      "\tson passed away on Aug 31st\n",
      "***Feature: No-hair-changes-OR-no-nail-changes-OR-no-temperature-intolerance\n",
      "***Patient note: pt 17y/m c/o palpitation since 2-3 months, episodes come and go, last for 3-4 minutes, no precipitating or releiving factors. last episode happened while playing basketball, pt felt chest pressure, and felt like almost passed out. pt has no chest pain, nause or vomitting, no tremors, no heat or cold intolerance, no changes in bladder or bowel habits. no changes in diet or polydypsia pt had changed college 7-8 months back, and has trouble adjusting to it, no rashes. pt has been taking 3-4 cups of coffee every day and red bull 4-5  in a week.\n",
      "no mood changes, no anxiety or panic attack\n",
      "ROS - nothing significant except as above\n",
      "allergy - none\n",
      "medication - pt takes aderral multiple times, last 2 days ago.\n",
      "PMH - none\n",
      "PSH - none\n",
      "SH - no smoking, occasional marijuana 5month ago, sexually active with girlfriend, uses condom, no STI\n",
      "FH - father had problems with heart and mother has hyperthyroid \n",
      "\n",
      "***Identified locations:\n",
      "\tno heat\n",
      "\tintolerance\n",
      "\tno\n",
      "\tcold intolerance\n",
      "***Feature: FHx-of-PUD-OR-Family-history-of-peptic-ulcer-disease\n",
      "***Patient note: Chad Hamilton is a 35 y/o male who comes into clinic with \"stomach problems\". The burning/gnawing pain is located at the mid-epigastrium and has been a 5/10 pain. It started 2 months ago, happening 1 time per week. The past two weeks it has been happening multiple times per day. It lasts an hour and is random in onset. he endorses belching, nausea, bloating after eating, and the pain wakes him up at night some times. He states his stools have been darker.\n",
      "ROS: negative \n",
      "Mast medical Hx: back related injury\n",
      "Past surgical hx: none\n",
      "Medications: tums and motrin\n",
      "FHx: paternal uncle had ulcer\n",
      "Social: 2-3 beers per week, .5 ppd since 15 y/o.\n",
      "\n",
      "***Identified locations:\n",
      "\tpaternal uncle had ulcer\n",
      "***Feature: Female\n",
      "***Patient note: Ms. Moore si a 45 yo woman with chief complaint of nervousness over the las 3-4 weeks. States that she is less hungry, but denies CP,SOB,palpitations,nausea,vomiting,diarrhea,abdominal pain/sweating/hot or cold intolerance/HA/hair loss. States that she has had difficulty falling asleep but that once she gets to sleep she has no issue with staying asleep. She has no daytime naps and works as an English professor with new lectures that began 2 weeks ago. She states that the nervousness is about anything and everything, and that she thinks about her kids, and her work and taking care of her mother. she states that it starts in the morning and is constant. She has no past medical/sugical history. 2 hospitalizations for the birth of her two children. Father had sudden MI at 65 and she takes tylenol occasionally for headaches. NKDA. drinks socially 1-2 drinks, no tobacco or drug use. Denies psychiatric history, depression or anxiety.\n",
      "***Identified locations:\n",
      "\twoman\n",
      "***Feature: No-suicidal-ideations\n",
      "***Patient note: 67yo woman presents with 3 weeks of difficulty falling asleep.  3 weeks ago her son was killed in an auto accident.  Pt says that it takes her a long time to fall asleep as shes thinking about her son, and she tosses and turns.  Pt has tried to take naps during the day, but has been unable to fall asleep.  Pt endorses a good support network including a loving and understanding husband.  Pt endorses increased appetite lately.  No change in weight.  Pt describes herself as sad, but not depressed.  She has diminished interest in hobbies, but is socializing.  Pt tried ambien to sleep but it didn't help.  Pt denies chest pain, SOB, fever, chills, nausea, vomiting, abdominal pain.  PMH sig for HTN treated with lisinopril and HCTZ, and pt is a 10yr breast CA survivor s/p lumpectomy.  Pt doesn't smoke, occassional EtOH use, no illicit drugs. Home BPs measurements have been good.\n",
      "***Identified locations:\n",
      "\t None\n",
      "***Feature: 20-year\n",
      "***Patient note: HPI: 20 y/o F complaines of progressive headache that started yesterday morning and is getting worse.  She describes the pain as all over her head and dull constant ache in nature.  Rated 8/10 w/ no radiation.  Walking, bending over and lights make the headache worse.  She has tried Ibuprofen, tylenol and sleeping by they have not improved the pain.  She also reports being achy all over since yesterday.  She reports significant nausea with her headaches.  She reports a subjective feeling of warmth and an achy neck.\n",
      "\n",
      "ROS: Denies travel, recent illnesses, previous episodes, cough, sick contacts, rashes, urinary or bowel changes\n",
      "Meds: Birth control pill\n",
      "FH: Mother with migraines, dad w/ high cholesterol\n",
      "Social: 2-3 beers on weekend, 3-4 joints per week, sexually active w/ boyriend and use condoms\n",
      "GyN: LMP 2 weeks ago with 28 day regular cycles\n",
      "***Identified locations:\n",
      "\t20 y/o\n",
      "***Feature: Male\n",
      "***Patient note: 17 yo male seen with verbal parental consent. He states that he has been having chest pain with deep breathing for the past 2 days. Pain is accompanied by muscle aches, nasal congestions, and subjective fevers. Pain was not relieved by tylenol or albuterol inhaler. Denies cough, weight loss, chest pain, falls, recent trauma, or drug use (e.g. cocaine). Denies sick contacts or recent travel.  \n",
      "PMH: Exerxcise induced asthma\n",
      "Medications: albuterol PRN for asthma, last use 6 months ago.\n",
      "NKDA\n",
      "Family hx: father with asthma and hyperlipidemia, grandpa with MI at 70 yo, denies hx of sickle cell, marfans, or ehler danlos\n",
      "Social hx: Senior in high school applying to college, works as carpenter assistant, rock climbs 2-3 times per week and stays active (denies falls), sexually active with one female partner uses condoms, drinks 1-2 beers & smoke 1-2 joints monthly at parties, denies tobacco or other drugs (e.g. cocaine, methamphetamine, MDMA)\n",
      "***Identified locations:\n",
      "\tmale\n"
     ]
    }
   ],
   "source": [
    "for idx, row in train.sample(10).iterrows():\n",
    "    print(f'***Feature: {row[\"feature_text\"]}')\n",
    "    print(f'***Patient note: {row[\"pn_history\"]}')\n",
    "    print('***Identified locations:')\n",
    "    if len(row['location']) == 0:\n",
    "        print('\\t None')\n",
    "    for (s, e) in row['location']:\n",
    "        print(f'\\t{row[\"pn_history\"][s:e]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'case_num', 'pn_num', 'feature_num', 'annotation', 'location_raw',\n",
       "       'feature_text', 'pn_history', 'location'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of a classifier for Myalgias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project.data.data_loaders import get_clean_train_data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sentence_transformers\n",
    "from sklearn.model_selection import train_test_split, GroupKFold\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 14300 entries, 0 to 14299\n",
      "Data columns (total 9 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   id            14300 non-null  object\n",
      " 1   case_num      14300 non-null  int64 \n",
      " 2   pn_num        14300 non-null  int64 \n",
      " 3   feature_num   14300 non-null  int64 \n",
      " 4   annotation    14300 non-null  object\n",
      " 5   location_raw  14300 non-null  object\n",
      " 6   feature_text  14300 non-null  object\n",
      " 7   pn_history    14300 non-null  object\n",
      " 8   location      14300 non-null  object\n",
      "dtypes: int64(3), object(6)\n",
      "memory usage: 1.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "train = get_clean_train_data()\n",
    "myalglias_data = train.query('feature_text == \"Myalgias\"')  # Manual inspection of features.csv shows this is the only way the feature comes up\n",
    "print(train.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can split our data into three types\n",
    "\n",
    "1. Those where we would not expect Myalgias to be present\n",
    "2. Those where we would expect it to be present but it is not\n",
    "3. Those where we would expect it to be present and it is\n",
    "\n",
    "The difference between types 2 and 3 is more important than the difference between types 1 and 3, since it requires more fine grain skills. Similiarly the difference between 1 and 2 is moreimportant than the difference between 1 and 3 since it precludes just predicting Myalgias if we see the general context of the partient. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, df in train.copy().groupby('pn_num'):\n",
    "    SHOULD_HAVE_MYALGIAS = 'Myalgias' in  df['feature_text'].unique()\n",
    "    DOES_HAVE_MYALGIAS = False \n",
    "    if SHOULD_HAVE_MYALGIAS and len(df.query('feature_text == \"Myalgias\"')['location'].iloc[0]) > 0:\n",
    "        DOES_HAVE_MYALGIAS = True\n",
    "    train.loc[df.index, 'should_have_myalgias'] = SHOULD_HAVE_MYALGIAS\n",
    "    train.loc[df.index, 'does_have_myalgias'] = DOES_HAVE_MYALGIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           location should_have_myalgias does_have_myalgias\n",
      "12603            []                 True              False\n",
      "12620            []                 True              False\n",
      "12637  [(370, 378)]                 True               True\n",
      "12654            []                 True              False\n",
      "12671  [(378, 388)]                 True               True\n",
      "...             ...                  ...                ...\n",
      "14218            []                 True              False\n",
      "14235            []                 True              False\n",
      "14252            []                 True              False\n",
      "14269            []                 True              False\n",
      "14286            []                 True              False\n",
      "\n",
      "[100 rows x 3 columns]\n",
      "      should_have_myalgias does_have_myalgias\n",
      "0                    False              False\n",
      "1                    False              False\n",
      "2                    False              False\n",
      "3                    False              False\n",
      "4                    False              False\n",
      "...                    ...                ...\n",
      "14295                 True              False\n",
      "14296                 True              False\n",
      "14297                 True              False\n",
      "14298                 True              False\n",
      "14299                 True              False\n",
      "\n",
      "[14200 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train.query('feature_text == \"Myalgias\"')[['location', 'should_have_myalgias', 'does_have_myalgias']])\n",
    "print(train.query('feature_text != \"Myalgias\"')[['should_have_myalgias', 'does_have_myalgias']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(row: pd.Series):\n",
    "    if not row['should_have_myalgias']:\n",
    "        return -1\n",
    "    elif not row['does_have_myalgias']:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "train['label'] = train.apply(get_label, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, all that matters is unique patient ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pn_num</th>\n",
       "      <th>pn_history</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41</td>\n",
       "      <td>17 Y/O M CAME TO THE CLINIC C/O HEART POUNDING...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46</td>\n",
       "      <td>Mr. Cleveland is a 17yo M who was consented by...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>82</td>\n",
       "      <td>17 yo M w/ no cardiac or arrhythmia PMH presen...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>HPI: Dillon Cleveland is an otherwise healthy ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pn_num                                         pn_history  label\n",
       "0      16  HPI: 17yo M presents with palpitations. Patien...     -1\n",
       "1      41  17 Y/O M CAME TO THE CLINIC C/O HEART POUNDING...     -1\n",
       "2      46  Mr. Cleveland is a 17yo M who was consented by...     -1\n",
       "3      82  17 yo M w/ no cardiac or arrhythmia PMH presen...     -1\n",
       "4     100  HPI: Dillon Cleveland is an otherwise healthy ...     -1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.drop_duplicates(subset=['pn_num']).reset_index(drop=True)[['pn_num', 'pn_history', 'label']]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    900\n",
       " 0     71\n",
       " 1     29\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pn_num', 'pn_history', 'label'], dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project.embedding.embedding_mechanism import CorpusEmbedder\n",
    "train['embedding'] = [CorpusEmbedder('all-mpnet-base-v2').embed(row[['pn_num', 'pn_history']]) for _, row in train.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "N_FOLDS = 2\n",
    "for _, df in train.copy().groupby('label'):\n",
    "    splits = [test_idx for (train_idx, test_idx) in KFold(n_splits=N_FOLDS, shuffle=True, random_state=42).split(df)]\n",
    "    for fold, split_idx in enumerate(splits):\n",
    "        train.loc[df.iloc[split_idx].index, 'fold'] = fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:00:18] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.86      0.79        36\n",
      "           1       0.44      0.27      0.33        15\n",
      "\n",
      "    accuracy                           0.69        51\n",
      "   macro avg       0.59      0.56      0.56        51\n",
      "weighted avg       0.65      0.69      0.66        51\n",
      "\n",
      "[17:00:18] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.77      0.74        35\n",
      "           1       0.27      0.21      0.24        14\n",
      "\n",
      "    accuracy                           0.61        49\n",
      "   macro avg       0.49      0.49      0.49        49\n",
      "weighted avg       0.59      0.61      0.60        49\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evanvogelbaum/opt/anaconda3/envs/PersonalCoding/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/evanvogelbaum/opt/anaconda3/envs/PersonalCoding/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "for fold in range(N_FOLDS):\n",
    "    fold_train = train.query('fold != @fold').query('label != -1')\n",
    "    fold_test = train.query('fold == @fold').query('label != -1')\n",
    "    X_train = np.stack(fold_train['embedding'].values)\n",
    "    X_test = np.stack(fold_test['embedding'].values)\n",
    "    y_train = fold_train['label'].values\n",
    "    y_test = fold_test['label'].values\n",
    "    model = XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.1, subsample=0.8, colsample_bytree=0.8, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id   case_num  pn_num  feature_num  annotation  location_raw  pn_history  location\n",
       "100  100       100     100          100         100           100         100         125\n",
       "200  200       200     200          200         200           200         200           4\n",
       "300  300       300     300          300         300           300         300           1\n",
       "700  700       700     700          700         700           700         700           1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_clean_train_data().groupby('feature_text').count().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "59811a21b6bdb79b6f1a3b21d01b6e64bb63c5c292a63d3c5cec461d0e515581"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('PersonalCoding')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
